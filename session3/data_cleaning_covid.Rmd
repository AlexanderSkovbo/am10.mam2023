---
title: "Session 3 (Data Visualization): Cleaning Covid Data"
author: "Your name goes here"
date: "`r Sys.Date()`"
output: 
    html_document:
      number_sections: true
      highlight: zenburn
      theme: flatly
      toc: yes
      toc_depth: 2
      toc_float:
        collapsed: false
      fontsize: 10pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(vroom)
library(janitor)
library(skimr)
library(vroom)
library(mice) 
library(VIM)

```

# Exploring BBC iPlayer streaming data

## Data

Covid 19 is a respiratory disease caused by a new virus that has been declared a pandemic by the World Health Organization. The purpose of this exercise is to investigate how the virus spread across the globe over time and to create a dashboard that allows a user to make their own investigation and reach their own conclusions. Not only do we want to see the various wavs of cases/hospitalisations/deaths, but we can also examine the race against covid with vaccinations around the world.

Any visualization exercise starts with good quality data. Arguably, the quality of data on the number of cases and deaths due to Covid 19 is not great because all countries have had limited testing capacity (so a lot of cases have been left undiagnosed /undetected) and some may actively distort numbers for political reasons. Any data mining project has such data limitations and bearing these in mind is important. One of the most reliable sources of up to date data is the *Our world in Data* website

```{r, get_data}
url <- "https://covid.ourworldindata.org/data/owid-covid-data.csv"
covid_data <- vroom(url) %>% 
  mutate(perc = round(100 * people_fully_vaccinated / population, 2))

```


Where to start analyzing a raw data set.

-   Transform data into technically correct data

1.  Each column has the same type of data that is consistent with what data in that column represents.
2.  Identify missing data.

-   Transform technically correct data into consistent data

1.  Handle missing values.
2.  Handle special values (eg., NA, N/A, inf..)
3.  Check for errors
4.  Check for outliers (eg., age=150)
5.  Check consistency between columns (eg., age=5, marital status=married)

## Data

The data file `raw_bbc_data.zip` contains information extracted from the BBC iPlayer database. The dataset was created by choosing approximately 10000 random viewers who watched something on iPlayer in January and then recording their viewing behaviour until the end of February. This means that customers who did not watch in January will not be in the dataset. Every row represents a viewing event. For every viewing event (i.e., every row), we have the following information:

a)  user_id -- a unique identifier for the viewer

b)  program_id and series_id -- these identify the program and the series that the program belongs to

c)  the programme's genre (e.g., drama, factual, news, sport, comedy, etc)

d)  the program duration in Excel time format (hh:mm:ss)

e)  the streaming start date/time of the event (in an unusual format that Excel does not recognise)

f)  how long the customer watched the program for (measured in milliseconds)

g)  Streaming id -- a unique identifier per streaming event For example, if the same person watches the same program twice, there will be two records that will have the same user_id, program_id, series_id, genre, and programme_duration, but will differ on streaming_id and start_date.

Given the way the data was created, during January the data is representative of what is watched on the iPlayer. After January the data is no longer representative as it is no longer a random sample of the people watching iPlayer content.

# Load and investigate data

Load the data.

```{r load_data}

# read_csv() will read and unzip the data file
bbc_data <- read_csv(here::here("data","raw_bbc_data.zip")) %>% 
  janitor::clean_names() 

#Now we can use other summary functions to have a more general idea about what is in the data
glimpse(bbc_data)

# always take a look at what's in the data first; skir::skim() is perhaps
# the most important tool you have. It will take a while to run
skimr::skim(bbc_data)


```

Make sure you understand what each variable represents. What units is each variable in (see above for column explanations)? Is data type of each column consistent with what you would expect?

Recall that our eventual goal is to estimate future usage of BBC iPlayer users. We will clean and process the data to prepare it for analysis.

# Technically correct data

In this step

1. Remove empty columns and rows and duplicates.
1. Check how many values are missing.
1. Correct data types, if necessary
1. Check min, max, and distribution of numerical values.
1. Names and distributions (counts) of categorical values.
1. Fix any additional irregularities.

## Initial data cleaning

Before we start looking at the contents of the data

i)  Remove empty columns and rows.

ii) Remove duplicates if there are any.

iii) Then check for missing data

```{r remove_empty}
#Remove empty columns and rows
originalData_process2 <- janitor::remove_empty(bbc_data, which = c("rows","cols"))

#Check for duplicates
dupes <- bbc_data %>% 
  janitor::get_dupes(streaming_id,start_date_time)

```

> Exercise 1 i) Are there any empty rows/colums in the data? ii) How about duplicated data? What should we do about these data points?

## Missing data

We should first check the missing values in the data.

To find the number of missing data points and for additional information we can use `skimr::skim()`

```{r investigate_data, message=FALSE}

skimr::skim(bbc_data)    


#or use the md.pattern function from MICE package
mice::md.pattern(bbc_data,rotate.names = T)
```

> Exercise 2: What columns have missing data? What other issues can you anticipate regarding the accuracy of the data?

We will deal with missing data below let's clean the data first.

## Data type conversion

> Exercise 3: Are all the columns in the formats they are supposed to be?

```{r data}
glimpse(bbc_data) 
```


### Time viewed format

Let's convert `time_viewed` and `program_duration` to minutes so that they use the same units.

What units are they in the original data? Let's start with `time_viewed`.

```{r convert_to_minutes}

bbc_data %>% 
  select(time_viewed) %>% 
  skim()


# Change it to minutes. Time viewed is given in milliseconds. Convert the view time to minutes. (1 minute is 60,000 miliseconds.)  
bbc_data <- bbc_data %>% 
  mutate(time_viewed_min = time_viewed/60000)

```

> Exercise 5: Check the distribution of time viewed using a box plot and a histogram. Are there any outliers?

```{r check_distribution_time_viewed}


```

View the top 50 records that have the longest time viewed. What do you observe?

```{r longest_time_viewed}

```


> Exercise 6: There were no missing values for program at the beginning but now there seems to be over 30,000 missing values. Why? Investigate.

Check the distribution of programme duration.

```{r check_program_duration}

```

View the top 50 records that have the longest programme duration. What do you observe?

```{r distribution_program_duration}

bbc_data %>% 
  slice_max(order_by=programme_duration, n=50)



```

### Categorical variables

```{r categorical_data}



```

> Exercise 7: There were no missing values for genre and seried_id but there seems to be a lot of data points missing these variables. Why? Investigate.

> Exercise 8: There are data points with missing genre and some with "N/A". What should we do about these?

# Consistent data

## Time viewed and program duration

> Exercise 9: What should we do about long view times and program durations?

## Cross column consistency

> Exercise 10: Are there any potential cross consistency issues in the data? Check for consistency!
